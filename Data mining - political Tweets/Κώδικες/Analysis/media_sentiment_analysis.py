# -*- coding: utf-8 -*-
"""media sentiment analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LgR4lpTGoQXFzYHgsClp3L2ZVtmh4O1Y
"""

import numpy as np
import pandas as pd
pd.set_option('display.max_rows', 40000)
pd.set_option('display.max_columns', 600)
pd.set_option('display.width', 3000)

import  matplotlib.pyplot as plt
from wordcloud import WordCloud

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.tokenize import regexp_tokenize 
from string import punctuation
from nltk.stem import WordNetLemmatizer 

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import re

import string
from string import punctuation

stopwords = stopwords.words('english')

cnn = pd.read_csv('cnn.csv')

"""**data preprocess**"""

#make all text lowercase
cnn['clean_text'] = cnn['text'].str.lower()
#remove words < 2 chars
cnn['clean_text'] = cnn['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))

#remove links with regex
p=re.compile('(www|http)\S+')
cnn['clean_text'] = cnn['clean_text'].apply(lambda x: re.sub(p,' ',x))
#remove @
cnn['clean_text'] = cnn['clean_text'].apply(lambda x: re.sub('@','',x))

#remove stopwords function
stops =  list(stopwords) + list(punctuation)

def remove_stops(text):
    text_no_stops = []
    for i in text:
        if i not in stops:
            if len(i) == 1:
                pass
            else:
                text_no_stops.append(i)
        else:
            pass
    return text_no_stops

#tokenize
cnn['text_tokenized'] = cnn['clean_text'].apply(lambda x: regexp_tokenize(x,"[\w']+"))

#actually remove stopwords
cnn['text_tokenized'] = cnn['text_tokenized'].apply(lambda x: remove_stops(x))

#lematization function
lemmatizer = nltk.stem.WordNetLemmatizer()
def lemmatize_text(text):
    lemmatized = []
    for word in text:
        lemmatized.append(lemmatizer.lemmatize(word))
    return lemmatized

#lemmatize
cnn['text_lemma'] = cnn['text_tokenized'].apply(lemmatize_text)

#create a string from lemmatized words
cnn['lemmatized_string'] = cnn['text_lemma'].apply(lambda x: ' '.join(x))

cnn.tail()

"""**create df for tweets about trump**"""

cnn_trump = pd.DataFrame()

keywords = ['trump']

for k in keywords :
    tmp = cnn[cnn['text'].str.lower().str.contains(k)]
    cnn_trump = cnn_trump.append(tmp)
    
    
    
cnn_trump.sort_values("date", inplace = True,ascending = False)   #sort by date
cnn_trump.drop_duplicates(subset="text",          #drop duplicate titles
                      keep = "first", inplace = True,)
cnn_trump = cnn_trump.reset_index(drop=True) #reset index
print("Trump related tweets: ",cnn_trump.shape[0])

cnn_trump['label'] = 'trump'

"""**create df for tweets about sanders** *italicized text*"""

cnn_sanders = pd.DataFrame()

keywords = ['bernie','sanders']

for k in keywords :
    tmp = cnn[cnn['text'].str.lower().str.contains(k)]
    cnn_sanders = cnn_sanders.append(tmp)
    
    
    
cnn_sanders.sort_values("date", inplace = True,ascending = False)   #sort by date
cnn_sanders.drop_duplicates(subset="text",          #drop duplicate titles
                      keep = "first", inplace = True,)
cnn_sanders = cnn_sanders.reset_index(drop=True) #reset index
print("Sanders related tweets: ",cnn_sanders.shape[0])

cnn_sanders['label'] = 'sanders'

index = cnn_trump.index
tr_rows = len(index)

index = cnn_sanders.index
san_rows = len(index)

"""**plot how many mentions each candidate gets**"""

mentions = [tr_rows,san_rows]

candidates =['Trump','Sanders']

plt.figure(figsize=(20,10))
fig=plt.bar(candidates,mentions,color=('orange','green'))
plt.title("Mention count for each candidate - CNN politics",fontsize=20)
plt.xlabel("Mentions",fontsize=20)
plt.ylabel('Candidate',fontsize=20)
plt.show()

"""**sentiment analysis**"""

from textblob import TextBlob

def analyze_sentiment(text):
    analysis = TextBlob(text)
    #return  analysis.sentiment.polarity
    
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity == 0:
        return 'neutral'
    else:
        return 'negative'

cnn_trump['sentiment'] = np.array([ analyze_sentiment(tweet) for tweet in cnn_trump['lemmatized_string'] ])

cnn_sanders['sentiment'] = np.array([ analyze_sentiment(tweet) for tweet in cnn_sanders['lemmatized_string'] ])

import seaborn as sns
sns.set(rc={'figure.figsize':(13,9)})
plt.style.use('fivethirtyeight')

ax = sns.countplot(x= 'sentiment', data=cnn_trump,palette='coolwarm').set_title('CNN: Sentiment of articles about Trump')
plt.show()

ax = sns.countplot(x= 'sentiment', data=cnn_sanders,palette='coolwarm').set_title('CNN: Sentiment of articles about Sanders')
plt.show()

"""###Analysis of articles recovered from rss feeds

articles recovered from: nytimes, bbc, rt, aljazzera,
"""

rss = pd.read_csv("rss_feed_english.csv")

rss.info()

rss.drop_duplicates(subset="link",keep = "first", inplace = True)

rss.title = rss.title.astype('str')
rss.summary = rss.summary.astype('str')

rss['text'] = rss.title + rss.summary

"""**the usual preproccessing**"""

#make all text lowercase
rss['clean_text'] = rss['text'].str.lower()

#remove words < 2 chars
rss['clean_text'] = rss['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))

#remove links with regex
p=re.compile('(www|http)\S+')
rss['clean_text'] = rss['clean_text'].apply(lambda x: re.sub(p,' ',x))
#remove @
rss['clean_text'] = rss['clean_text'].apply(lambda x: re.sub('@','',x))

#tokenize
rss['text_tokenized'] = rss['clean_text'].apply(lambda x: regexp_tokenize(x,"[\w']+"))

#actually remove stopwords
rss['text_tokenized'] = rss['text_tokenized'].apply(lambda x: remove_stops(x))

#lemmatize
rss['text_lemma'] = rss['text_tokenized'].apply(lemmatize_text)

#create a string from lemmatized words
rss['lemmatized_string'] = rss['text_lemma'].apply(lambda x: ' '.join(x))

rss_trump = pd.DataFrame()

keywords = ['trump']

for k in keywords :
    tmp = rss[rss['text'].str.lower().str.contains(k)]
    rss_trump = rss_trump.append(tmp)
    
    
    
rss_trump.sort_values("date", inplace = True,ascending = False)   #sort by date
rss_trump = rss_trump.reset_index(drop=True) #reset index
print("Trump related articles: ",rss_trump.shape[0])

rss_sanders = pd.DataFrame()

keywords = ['sanders']

for k in keywords :
    tmp = rss[rss['text'].str.lower().str.contains(k)]
    rss_sanders = rss_sanders.append(tmp)
    
    
    
rss_sanders.sort_values("date", inplace = True,ascending = False)   #sort by date
rss_sanders = rss_sanders.reset_index(drop=True) #reset index
print("Sanders related articles: ",rss_sanders.shape[0])

rss.tail()

rss_biden = pd.DataFrame()

keywords = ['biden']

for k in keywords :
    tmp = rss[rss['text'].str.lower().str.contains(k)]
    rss_biden = rss_biden.append(tmp)
    
    
    
rss_biden.sort_values("date", inplace = True,ascending = False)   #sort by date
rss_biden = rss_biden.reset_index(drop=True) #reset index
print("Biden related articles: ",rss_biden.shape[0])

index = rss_trump.index
tr_rows = len(index)
index = rss_sanders.index
san_rows = len(index)
index = rss_biden.index
bid_rows = len(index)

mentions = [tr_rows,san_rows]
candidates = ['Trump','Sanders']

plt.figure(figsize=(20,10))
fig=plt.bar(candidates,mentions,color=('orange','green'))
plt.title("Mention count for each candidate - articles from multiple sources",fontsize=20)
plt.xlabel("Mentions",fontsize=20)
plt.ylabel('Candidate',fontsize=20)
plt.show()

rss_trump['sentiment'] = np.array([ analyze_sentiment(tweet) for tweet in rss_trump['lemmatized_string'] ])

rss_sanders['sentiment'] = np.array([ analyze_sentiment(tweet) for tweet in rss_sanders['lemmatized_string'] ])

rss_biden['sentiment'] = np.array([ analyze_sentiment(tweet) for tweet in rss_biden['lemmatized_string'] ])

ax = sns.countplot(x= 'sentiment', data=rss_trump,palette='coolwarm').set_title('Articles from multiple sources: Trump')
plt.show()

ax = sns.countplot(x= 'sentiment', data=rss_biden,palette='coolwarm').set_title('Articles from multiple sources: Biden')
plt.show()

ax = sns.countplot(x= 'sentiment', data=rss_sanders,palette='coolwarm').set_title('Articles from multiple sources: Sanders')
plt.show()

"""**clustering**"""

rss_trump.head()

# tfidf vectorizer of scikit learn

vectorizer = TfidfVectorizer(stop_words=stopwords,use_idf = True, ngram_range=(1,2))
#vectorize lemmatized tweets
X = vectorizer.fit_transform(rss_sanders['lemmatized_string'])
terms = vectorizer.get_feature_names()

from sklearn.cluster import KMeans

#K-means clustering
num_clusters = 2
km = KMeans(n_clusters=num_clusters,init='k-means++')
labels = km.fit_predict(X)

print("K-means clustering: \n")
asc_order_centroids = km.cluster_centers_.argsort()#[:, ::-1]
order_centroids = asc_order_centroids[:,::-1]
terms = vectorizer.get_feature_names()
for i in range(num_clusters):
    print ("Cluster %d:" % i)
    for ind in order_centroids[i, :10]:
        print (' %s' % terms[ind])
    print('\n')

rss_trump['cluster'] = labels