# -*- coding: utf-8 -*-
"""bernie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cDIBuQFiZTOrR79o26CKVbK-oeMpUSor

**imports**
"""

import numpy as np
import pandas as pd
pd.set_option('display.max_rows', 40000)
pd.set_option('display.max_columns', 600)
pd.set_option('display.width', 3000)

import  matplotlib.pyplot as plt
from wordcloud import WordCloud

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.tokenize import regexp_tokenize 
from string import punctuation
from nltk.stem import WordNetLemmatizer 

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

import re

import string
from string import punctuation

stopwords = stopwords.words('english')

#mount google drive
from google.colab import drive
drive.mount('/content/drive')

"""**create dataframe for bernie's tweets**"""

path = '/content/drive/My Drive/datasets/bernie.csv'
df_bernie = pd.read_csv(path)
#drop duplicate articles on original df_bernieset
df_bernie.drop_duplicates(subset="text",keep = "first", inplace = True)
print("Total retrieved articles: ",df_bernie.shape[0])
print(df_bernie.shape)
df_bernie.head()

"""**text preproccess**"""

#make all text lowercase
df_bernie['clean_text'] = df_bernie['text'].str.lower()

#remove words < 2 chars
df_bernie['clean_text'] = df_bernie['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))

#remove links with regex
p=re.compile('(www|http)\S+')
df_bernie['clean_text'] = df_bernie['clean_text'].apply(lambda x: re.sub(p,' ',x))
#remove @
df_bernie['clean_text'] = df_bernie['clean_text'].apply(lambda x: re.sub('@','',x))

#remove punctuation

#remove stopwords function
stops =  list(stopwords) + list(punctuation)

def remove_stops(text):
    text_no_stops = []
    for i in text:
        if i not in stops:
            if len(i) == 1:
                pass
            else:
                text_no_stops.append(i)
        else:
            pass
    return text_no_stops

#tokenize
df_bernie['text_tokenized'] = df_bernie['clean_text'].apply(lambda x: regexp_tokenize(x,"[\w']+"))

#actually remove stopwords
df_bernie['text_tokenized'] = df_bernie['text_tokenized'].apply(lambda x: remove_stops(x))

#lematization function
lemmatizer = nltk.stem.WordNetLemmatizer()
def lemmatize_text(text):
    lemmatized = []
    for word in text:
        lemmatized.append(lemmatizer.lemmatize(word))
    return lemmatized

#lemmatize

df_bernie['text_lemma'] = df_bernie['text_tokenized'].apply(lemmatize_text)

#create a string from lemmatized words

df_bernie['lemmatized_string'] = df_bernie['text_lemma'].apply(lambda x: ' '.join(x))

#check the dataframe
df_bernie.head()

# tfidf vectorizer of scikit learn

vectorizer = TfidfVectorizer(stop_words=stopwords,use_idf = True, ngram_range=(1,2))
#vectorize lemmatized tweets
X = vectorizer.fit_transform(df_bernie['lemmatized_string'])
terms = vectorizer.get_feature_names()

df_bernie.head()

"""**K-MEANS clustering**

**find best K by using elbow rule**
"""

'''distortions = []
    K = range(150,160)
    for k in K:
        km = KMeans(n_clusters=k)
        km.fit(X)
        distortions.append(km.inertia_)'''

''' plt.figure(figsize=(16,8))
    plt.plot(K, distortions, 'bx-')
    plt.xlabel('k')
    plt.ylabel('Distortion')
    plt.title('The Elbow Method showing the optimal k')
    plt.show()'''

df_bernie.head()

#K-means clustering
num_clusters = 40
km = KMeans(n_clusters=num_clusters,init='k-means++')
labels = km.fit_predict(X)

print("K-means clustering: \n")
asc_order_centroids = km.cluster_centers_.argsort()#[:, ::-1]
order_centroids = asc_order_centroids[:,::-1]
terms = vectorizer.get_feature_names()
for i in range(num_clusters):
    print ("Cluster %d:" % i)
    for ind in order_centroids[i, :10]:
        print (' %s' % terms[ind])
    print('\n')

#add cluster label to dataframe
df_bernie['cluster'] = labels

df_bernie.head()

"""**plot cluster size**"""

import seaborn as sns
sns.set(rc={'figure.figsize':(13,9)})
plt.style.use('fivethirtyeight')

ax = sns.countplot(x= 'cluster', data=df_bernie)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
plt.show()

"""**plot the clusters**

**sentiment analysis**
"""

from textblob import TextBlob

def analyze_sentiment(text):
    analysis = TextBlob(text)
    #return  analysis.sentiment.polarity
    
    if analysis.sentiment.polarity > 0:
        return "positive"
    elif analysis.sentiment.polarity == 0:
        return "neutral"
    else:
        return "negative"

df_bernie['sentiment'] = np.array([ analyze_sentiment(tweet) for tweet in df_bernie['lemmatized_string'] ])

df_bernie.head()

"""**persentage of positive/negative/neutral tweets**"""

ax = sns.countplot(x= 'sentiment', data=df_bernie)
plt.show()

"""**find popular hashtags**"""

#create new df hashtags
hashtags = df_bernie["text"].str.extractall(r'(\#\w+)')[0].value_counts().reset_index()

hashtags.columns=['tag','count']

ax = sns.barplot(x = "count" , y = "tag" ,
                 data = hashtags[:25] , palette = "seismic",
                 linewidth = 1 , edgecolor = "k")
plt.grid(True)
for i,j in enumerate(hashtags["count"][:25].values) :
    ax.text(3,i,j,fontsize = 10,color = "white")
plt.title("25 more popular hashtags used by Bernie Sanders")

"""**popular account handlers used**"""

#create new df accounts
accounts = df_bernie["text"].str.extractall(r'(\@\w+)')[0].value_counts().reset_index()

accounts.columns=['username','count']

ax = sns.barplot(x = "count" , y = "username" ,
                 data = accounts[:25] , palette = "seismic",
                 linewidth = 1 , edgecolor = "k")
plt.grid(True)
for i,j in enumerate(accounts["count"][:25].values) :
    ax.text(3,i,j,fontsize = 10,color = "white")
plt.title("25 more popular usernames mentioned by Bernie Sanders")

"""**network analysis**"""

from sklearn.feature_extraction.text import CountVectorizer
import networkx as nx

def network_tweets(df,frequency,color,title) :
    #documents
    documents  = df["lemmatized_string"].tolist()
    vectorizer = CountVectorizer()
    vec        = vectorizer.fit_transform(documents)
    vec_t      = vectorizer.fit_transform(documents).transpose()
    
    #adjecency matrix for words
    adj_mat    = pd.DataFrame((vec_t * vec).toarray(),
                              columns = vectorizer.get_feature_names(),
                              index    = vectorizer.get_feature_names()
                             )
    # #stacking combinations
    adj_mat_stack   = adj_mat.stack().reset_index()
    adj_mat_stack.columns = ["link_1","link_2","count"]
    
    #drop same word combinations
    adj_mat_stack   = adj_mat_stack[adj_mat_stack["link_1"] !=
                                    adj_mat_stack["link_2"]] 
    
    #subset dataframe with combination count greater than x times
    network_sub = adj_mat_stack[adj_mat_stack["count"] > frequency]
    
    #plot network
    H = nx.from_pandas_edgelist(network_sub,"link_1","link_2",["count"],
                                create_using = nx.DiGraph())

    ax = plt.figure(figsize = (18,18))
    nx.draw(H,with_labels = True,alpha = .7,node_shape = "H",
            width = 1,node_color = color,
            font_weight = "bold",style = "solid", arrowsize = 15 ,
            font_color = "white",linewidths = 10,edge_color = "grey",
            node_size = 1300,pos = nx.kamada_kawai_layout(H))
    plt.title(title,color = "white")
    ax.set_facecolor("k")

network_tweets(df_bernie,80,"#FF3300","Network analysis of tweet words - Bernie Sanders")

"""**tweets per year plot**"""

#create column year
df_bernie['year'] = df_bernie['date'].apply(lambda buf: int(buf[:4]))

sns.countplot(x='year',data=df_bernie, palette = 'coolwarm')

"""**Tweet numbers incrases dramatically at 2016,2019 -> Democratic Party elections**

**WordCloud Generation**
"""

dummy = df_bernie[df_bernie['cluster']==15]

text = df_bernie.lemmatized_string

wordcloud = WordCloud(
    width = 2000,
    height = 1000,
    background_color = 'black',
    stopwords = stopwords).generate(str(text))
fig = plt.figure(
    figsize = (30, 20),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

"""**create new df for tweets refering to trump**"""

keywords = ['trump']
trump = pd.DataFrame()

for k in keywords :
  tmp = df_bernie[df_bernie['text'].str.lower().str.contains(k)]
  trump = trump.append(tmp)

trump = trump.reset_index(drop=True)

trump.drop("cluster", axis=1, inplace=True)

"""**sentiment of tweets regarding trump**"""

ax = sns.countplot(x= 'sentiment', data=trump)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
plt.show()

network_tweets(trump,40,"#FF3300","Network analysis of tweet words - Bernie Sanders -> Donald Trump")

"""**clustering on tweets about trump**"""

vectorizer = TfidfVectorizer(stop_words=stopwords,max_features=10000, max_df = 0.5, use_idf = True, ngram_range=(1,3))
X = vectorizer.fit_transform(trump['lemmatized_string'])

num_clusters = 30
km = KMeans(n_clusters=num_clusters,init='k-means++')
labels = km.fit_predict(X)
trump['cluster'] = labels

print("K-means clustering: \n")
asc_order_centroids = km.cluster_centers_.argsort()#[:, ::-1]
order_centroids = asc_order_centroids[:,::-1]
terms = vectorizer.get_feature_names()
for i in range(num_clusters):
    print ("Cluster %d:" % i)
    for ind in order_centroids[i, :10]:
        print (' %s' % terms[ind])
    print('\n')

sns.countplot(x='cluster',data=trump)

"""**visualize clusters**"""

clusters = km.labels_.tolist()

import umap
embedding = umap.UMAP(n_neighbors=100, min_dist=0.5, random_state=12).fit_transform(X)

plt.figure(figsize=(10,8))
plt.scatter(embedding[:, 0], embedding[:, 1], 
c = clusters,
s = 10, # size
edgecolor='none')
plt.show()